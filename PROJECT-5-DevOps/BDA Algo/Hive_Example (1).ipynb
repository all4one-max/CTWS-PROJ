{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hive Example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD1zvV9JtRsN"
      },
      "source": [
        "# Loading and Saving Data in Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXcc49lmUYgz"
      },
      "source": [
        "Collab Only code:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar -xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "Dm-SqeIOb-Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbRIu4xkU2qN"
      },
      "source": [
        "**Not on Colab you should start form HERE:**\n",
        "\n",
        "Reading a text file textFile() in Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMHp9gMPUyPr"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPoJs6cguwyh"
      },
      "source": [
        "Loading all the .md files in one directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l9c1Jdp5O6c"
      },
      "source": [
        "# **Hive Example**\n",
        "\n",
        "Using Hive to create and read a table - Simple Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOcCViaN4cwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0f0590e-281d-4181-f9b2-ab08509ef780"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import HiveContext\n",
        "sqlContext = HiveContext(sc)\n",
        "test_list = [('A', 25),('B', 20),('C', 25),('D', 18)]\n",
        "rdd = sc.parallelize(test_list)\n",
        "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
        "schemaPeople = sqlContext.createDataFrame(people)\n",
        "# Register it as a temp table\n",
        "sqlContext.registerDataFrameAsTable(schemaPeople, \"test_table\")\n",
        "sqlContext.sql(\"show tables\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/context.py:604: FutureWarning: HiveContext is deprecated in Spark 2.0.0. Please use SparkSession.builder.enableHiveSupport().getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+-----------+\n",
            "|namespace| tableName|isTemporary|\n",
            "+---------+----------+-----------+\n",
            "|         |test_table|       true|\n",
            "+---------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWcwTJx_5cYW"
      },
      "source": [
        "Let's query the table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaX5OJaz5eAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a59044ce-90a4-4a81-9990-5cffc44aa989"
      },
      "source": [
        "sqlContext.sql(\"Select * from test_table\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|   A| 25|\n",
            "|   B| 20|\n",
            "|   C| 25|\n",
            "|   D| 18|\n",
            "+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIcPB7Mi5-Zu"
      },
      "source": [
        "**Load a JSON file with Hive and use SQL on it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKD0QGQn6G3H"
      },
      "source": [
        "## Colab code only - DO NOT run outsie of colab\n",
        "from google.colab import files  \n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J7JUJy16OJF"
      },
      "source": [
        "Let's load example1.json with Hive a do a Select Statement on it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axQ3wgQS6aLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55597669-2870-40be-b270-239be4d5ffdd"
      },
      "source": [
        "from pyspark.sql import HiveContext\n",
        "hiveCtx = HiveContext(sc)\n",
        "ex1 = hiveCtx.read.option(\"multiline\", \"true\").json(\"exam.json\")\n",
        "ex1.registerTempTable(\"ex1\")\n",
        "results = hiveCtx.sql(\"SELeCT * FROM ex1\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/context.py:604: FutureWarning: HiveContext is deprecated in Spark 2.0.0. Please use SparkSession.builder.enableHiveSupport().getOrCreate() instead.\n",
            "  FutureWarning\n",
            "/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py:140: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+---+---+\n",
            "|       FN|    LN| RN| id|\n",
            "+---------+------+---+---+\n",
            "|   Saurav|   Jha| 65|  1|\n",
            "|Deepanshu|Google|  3|  2|\n",
            "|   Shivam|  Modi| 21|  3|\n",
            "+---------+------+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hiveCtx.sql(\"show tables\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk-347F_cDjH",
        "outputId": "4664f8c0-e185-41c6-b35f-9b7895505bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+-----------+\n",
            "|namespace| tableName|isTemporary|\n",
            "+---------+----------+-----------+\n",
            "|         |       ex1|       true|\n",
            "|         |test_table|       true|\n",
            "+---------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}