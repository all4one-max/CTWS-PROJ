{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HIVE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y4uBGISc8stL"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar -xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "# spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Python Spark SQL Hive\").enableHiveSupport().getOrCreate()"
      ],
      "metadata": {
        "id": "IQ2ByFZ5bvSm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")\n",
        "spark.sql(\"show tables\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rujoUOcsa-E7",
        "outputId": "6556eb7b-ad38-4c74-d752-c89dcfe93c39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|  default|      src|      false|\n",
            "+---------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM src\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkSQk0pabEq6",
        "outputId": "9f51b293-0fa0-42b9-986e-6286afbbe8c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|key|value|\n",
            "+---+-----+\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('INSERT INTO src (key, value) VALUES (238, \"val_238\")')\n",
        "spark.sql('INSERT INTO src (key, value) VALUES (126, \"val_126\")')\n",
        "spark.sql('INSERT INTO src (key, value) VALUES (235, \"val_235\")')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOqQxs09bG3c",
        "outputId": "3bdf49f0-bdab-42ee-e613-1cf8a96b4fa0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM src\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50ffFQy_bKbA",
        "outputId": "2e104a1d-bce9-492c-ef92-4b135953ead1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "|key|  value|\n",
            "+---+-------+\n",
            "|238|val_238|\n",
            "|235|val_235|\n",
            "|126|val_126|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT COUNT(*) FROM src\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgEHgVPKbNuE",
        "outputId": "2ebfd0ae-e80f-4d73-ede7-b9e656c5d566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|       3|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key < 200 ORDER BY key\")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UprtwL-bPuh",
        "outputId": "a316914b-0d97-4d31-81b5-a728b3d2f19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "|key|  value|\n",
            "+---+-------+\n",
            "|126|val_126|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stringsDS = sqlDF.rdd.map(lambda row: \"Key: %d, Value: %s\" % (row.key, row.value))\n",
        "for record in stringsDS.collect():\n",
        "    print(record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cat7KHWGbSNy",
        "outputId": "faf08eb4-07fe-4ba4-f2da-2ec8ffa7eab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key: 126, Value: val_126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "Record = Row(\"key\", \"value\")\n",
        "recordsDF = spark.createDataFrame([Record(i, \"val_\" + str(i)) for i in range(1, 101)])\n",
        "recordsDF.createOrReplaceTempView(\"records\")\n",
        "\n",
        "recordsDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO8vlII0bUKS",
        "outputId": "69beff76-eb03-466c-e1bd-a5679d31efc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "|key| value|\n",
            "+---+------+\n",
            "|  1| val_1|\n",
            "|  2| val_2|\n",
            "|  3| val_3|\n",
            "|  4| val_4|\n",
            "|  5| val_5|\n",
            "|  6| val_6|\n",
            "|  7| val_7|\n",
            "|  8| val_8|\n",
            "|  9| val_9|\n",
            "| 10|val_10|\n",
            "| 11|val_11|\n",
            "| 12|val_12|\n",
            "| 13|val_13|\n",
            "| 14|val_14|\n",
            "| 15|val_15|\n",
            "| 16|val_16|\n",
            "| 17|val_17|\n",
            "| 18|val_18|\n",
            "| 19|val_19|\n",
            "| 20|val_20|\n",
            "+---+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"show tables\").show()\n",
        "spark.sql(\"SELECT * FROM records\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bRCuFuzbWft",
        "outputId": "10e413dc-1e84-4bb7-b8f7-2ca298da27a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|  default|      src|      false|\n",
            "|         |  records|       true|\n",
            "+---------+---------+-----------+\n",
            "\n",
            "+---+------+\n",
            "|key| value|\n",
            "+---+------+\n",
            "|  1| val_1|\n",
            "|  2| val_2|\n",
            "|  3| val_3|\n",
            "|  4| val_4|\n",
            "|  5| val_5|\n",
            "|  6| val_6|\n",
            "|  7| val_7|\n",
            "|  8| val_8|\n",
            "|  9| val_9|\n",
            "| 10|val_10|\n",
            "| 11|val_11|\n",
            "| 12|val_12|\n",
            "| 13|val_13|\n",
            "| 14|val_14|\n",
            "| 15|val_15|\n",
            "| 16|val_16|\n",
            "| 17|val_17|\n",
            "| 18|val_18|\n",
            "| 19|val_19|\n",
            "| 20|val_20|\n",
            "+---+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('INSERT INTO src (key, value) VALUES (12, \"val_12\")')\n",
        "spark.sql('INSERT INTO src (key, value) VALUES (23, \"val_23\")')\n",
        "spark.sql('INSERT INTO src (key, value) VALUES (75, \"val_75\")')\n",
        "spark.sql('INSERT INTO src (key, value) VALUES (25, \"val_25\")')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XXGVqMlbZQz",
        "outputId": "0792fbc0-5375-4941-f2f3-063e2901df68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM records r JOIN src s ON r.key = s.key\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB976jAXbbMD",
        "outputId": "07887e04-134c-4bb3-868a-b401343ea1cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+------+\n",
            "|key| value|key| value|\n",
            "+---+------+---+------+\n",
            "| 12|val_12| 12|val_12|\n",
            "| 23|val_23| 23|val_23|\n",
            "| 25|val_25| 25|val_25|\n",
            "| 75|val_75| 75|val_75|\n",
            "+---+------+---+------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}